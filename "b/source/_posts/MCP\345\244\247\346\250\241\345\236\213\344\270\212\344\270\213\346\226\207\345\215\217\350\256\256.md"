---
title: MCP 大模型上下文协议
date: 2025-07-10 10:00:00
tags: [AI, 大模型]
categories: [AI, MCP]
cover: /images/openai.png
---

## 背景

近年来，大模型的应用场景越来越广泛，但如何在多种应用和模型之间共享上下文信息仍是难题。**MCP（Model Context Protocol）** 旨在解决模型在推理过程中上下文传递与管理的不统一问题，让不同框架、不同环境下的模型都能以统一的方式获取历史对话与额外的系统提示。

## 协议目标

1. **统一格式**：定义一套通用的上下文描述方式，便于在多种模型之间传递。
2. **灵活扩展**：支持自定义字段，可按需扩展额外信息（如用户标识、时间戳等）。
3. **框架无关**：无论模型使用何种推理引擎，都可以解析并利用 MCP 中的数据。

## 报文结构

MCP 报文整体采用 JSON 格式，主要包含三大部分：

```json
{
  "version": "1.0",
  "messages": [ ... ],
  "meta": { ... }
}
```

- **version**：协议版本号，便于未来向后兼容。
- **messages**：对话消息数组，每条消息都包含角色、内容以及可选的额外属性。
- **meta**：描述对话整体的元数据，例如会话 ID、过期时间等。

### messages 字段

单条消息通常包括：

```json
{
  "role": "user" | "assistant" | "system",
  "content": "消息内容",
  "timestamp": 1691234567
}
```

其中 `role` 指明消息的来源，`content` 为正文，`timestamp` 可选，用于记录时间。

### meta 字段

meta 可用于记录一些全局信息，例如：

```json
{
  "session_id": "abc123",
  "expire_at": 1691299999,
  "custom": {
    "locale": "zh_CN"
  }
}
```

开发者可以在 `custom` 字段中存放与业务相关的自定义数据。

## 实现示例

在实际接入时，只需构造符合 MCP 结构的上下文即可传入大模型。以某 Python SDK 为例：

```python
context = {
    "version": "1.0",
    "messages": [
        {"role": "system", "content": "你是一个友好的助手"},
        {"role": "user", "content": "你好"}
    ],
    "meta": {"session_id": "abc123"}
}

response = llm.generate(context)
```

模型在收到 `context` 后，即可根据其中的历史对话与系统提示产生回复。

## 展望

随着 AI 技术的发展，统一的上下文协议有助于模型之间的协作与生态构建。MCP 作为一种轻量级规范，既能覆盖常见需求，又允许开发者按需扩展。未来，MCP 还可与更多推理框架和服务集成，形成更完善的标准。

